
Kimeraâ€‘SWM Toy Prototype **v0.7.3**
===================================

This repository holds a minimal, CPUâ€‘only prototype implementing the
core concepts defined in *Kimeraâ€‘SWM Microâ€‘Specification v0.1*.

Features batch processing with latency and memory monitoring for large datasets.
**New in v0.7.3**: CLS lattice storage with cls_event tracking and time-decay weighting (Ï„ = 14 days).
**New in v0.7.2**: EchoForm core implementation with stable hashing and enhanced observability.
**New in v0.7.0**: Negation fix with environment variable control and comprehensive benchmarking.
**New in v0.3.0**: Memory-optimized streaming for handling 10k+ pairs efficiently.

## Repository Structure

```
kimera_swm_toy/
â”œâ”€â”€ src/kimera/              # Core source code
â”‚   â”œâ”€â”€ identity.py         # Unified Identity system (Geoids + SCARs)
â”‚   â”œâ”€â”€ storage.py          # DuckDB-based persistent storage
â”‚   â”œâ”€â”€ cls.py              # Continuous Learning System
â”‚   â”œâ”€â”€ reactor.py          # Reactor and multiprocessing
â”‚   â”œâ”€â”€ echoform.py         # EchoForm implementation
â”‚   â”œâ”€â”€ geoid.py            # Geoid-specific functionality
â”‚   â”œâ”€â”€ entropy.py          # Entropy calculations
â”‚   â””â”€â”€ cache.py            # Caching utilities
â”œâ”€â”€ tests/                   # Comprehensive test suite
â”‚   â”œâ”€â”€ unit/               # Unit tests (pure, isolated)
â”‚   â”œâ”€â”€ integration/        # Integration tests (multi-component)
â”‚   â””â”€â”€ functional/         # End-to-end functional tests
â”œâ”€â”€ docs/                   # Documentation and guides
â”‚   â”œâ”€â”€ guides/             # Implementation guides
â”‚   â”‚   â”œâ”€â”€ scar_guide.md   # SCAR implementation guide
â”‚   â”‚   â”œâ”€â”€ cls_lattice_guide.md # CLS lattice integration
â”‚   â”‚   â”œâ”€â”€ vault_guide.md  # Vault usage guide
â”‚   â”‚   â””â”€â”€ benchmark_guide.md # Benchmarking guide
â”‚   â”œâ”€â”€ status/             # Development status files
â”‚   â””â”€â”€ ARCHIVE/           # Historical development documents
â”œâ”€â”€ scripts/               # Utility and maintenance scripts
â”œâ”€â”€ vault/                 # Vault subsystem for persistent storage
â”œâ”€â”€ benchmarks/            # Performance benchmarking suite
â””â”€â”€ .github/workflows/     # CI/CD configuration
```

Quickâ€‘start (Windowsâ€‘friendly)
------------------------------
```bash
# Install dependencies
python -m pip install --upgrade pip
pip install poetry==1.8.2
poetry install --sync

# Update lockfile after dependency changes
poetry lock
poetry install

# Run tests using the new Makefile
make test              # Run all tests
make test-unit         # Run unit tests only
make test-integration  # Run integration tests only
make test-functional   # Run functional tests only

# Alternative: use pytest directly
pytest tests/unit/     # Unit tests
pytest tests/integration/  # Integration tests
pytest tests/functional/   # Functional tests

# Code quality
make lint              # Check code quality
make format            # Format code

# Clean up
make clean             # Remove cache files
```
poetry run pytest tests/ -v

# Run specific test categories
poetry run pytest tests/unit/ -v          # Unit tests
poetry run pytest tests/integration/ -v   # Integration tests
poetry run pytest tests/functional/ -v    # Functional tests

# Verify SCAR implementation
python scripts/verify_scar_implementation.py

# Run demo with toy dataset (22 rows)
poetry run python -m kimera.demo

# Generate larger dataset (2K rows, tri-lingual)
poetry run python scripts/build_dataset.py --rows 2000 --lang en,fr,ar --mode online --out data/contradictions_2k.csv

# Run demo with larger dataset - choose your platform:

# Option 1: CLI argument (cross-platform)
poetry run python -m kimera.demo data/contradictions_2k.csv --chunk 300

# Option 2: Environment variable (Windows PowerShell)
$env:KIMERA_DATASET_PATH = "data/contradictions_2k.csv"
poetry run python -m kimera.demo --chunk 300

# Option 3: Environment variable (Windows CMD)
set KIMERA_DATASET_PATH=data\contradictions_2k.csv
poetry run python -m kimera.demo --chunk 300

# Option 4: Environment variable (Unix/Linux/Mac)
KIMERA_DATASET_PATH=data/contradictions_2k.csv poetry run python -m kimera.demo --chunk 300
```

## Development Workflow

### New Organized Structure
This repository has been reorganized for better maintainability:

- **Tests are categorized**: Unit, integration, and functional tests are now properly separated
- **Documentation is centralized**: All guides are in `docs/guides/`
- **Scripts are organized**: Utility scripts moved to `scripts/`
- **Makefile replaces run_* scripts**: Single entry point for all development tasks

### Running Tests
```bash
# Run all tests
make test

# Run specific test categories
make test-unit         # Fast, isolated unit tests
make test-integration  # Multi-component integration tests  
make test-functional   # End-to-end system tests

# Run specific test files
pytest tests/unit/test_identity.py
pytest tests/integration/test_scar_functionality.py
pytest tests/functional/test_benchmark_quick.py
```

### Code Quality
```bash
# Check code quality
make lint

# Format code
make format

# Clean up temporary files
make clean
```

### Benchmarking
```bash
# Quick benchmark (Kimera only)
python -m benchmarks.llm_compare --kimera-only --max-pairs 10

# Full benchmark (requires OpenAI API key)
python -m benchmarks.llm_compare --max-pairs 50
```

## Legacy Commands (Deprecated)

The numerous `run_*.py` scripts have been replaced by the Makefile. For reference:
- `run_tests.py` â†’ `make test`
- `run_quick_test.py` â†’ `make test-unit`
- `run_verification.py` â†’ `make test-functional`
- `run_benchmark_quick.py` â†’ `python -m benchmarks.llm_compare --kimera-only`

### Batch Processing
The demo now includes performance monitoring:
- **Latency tracking**: Average ms per reactor cycle
- **Memory monitoring**: Memory delta during processing  
- **Progress bars**: Visual feedback for large datasets
- **Configurable chunks**: Tune batch size for your hardware

Expected output:
```
Reactor cycles 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:05<00:00,  2.50chunk/s]
Loaded 3996 geoids
Pairs processed: 1998
Latency (ms): 5123.4
Î” Memory (MB): 18.7
Scars logged : 1650
```

GPT-4o Benchmark (Phase 2.1 - Memory Optimized)
-----------------------------------------------
Compare Kimera's contradiction detection against GPT-4o with enhanced memory efficiency:

```bash
# Set your OpenAI API key
export OPENAI_API_KEY="sk-..."  # Unix/Mac
# or
$env:OPENAI_API_KEY = "sk-..."  # PowerShell

# Run full benchmark (Kimera + GPT-4o) - now supports large datasets!
poetry run python -m benchmarks.llm_compare data/toy_contradictions.csv --max-pairs 50

# Large dataset benchmark (automatic streaming)
poetry run python -m benchmarks.llm_compare data/contradictions_2k.csv --max-pairs 1000

# Use different GPT model
poetry run python -m benchmarks.llm_compare --model gpt-4o --max-pairs 25

# Run Kimera-only benchmark (no API key needed)
poetry run python -m benchmarks.llm_compare --kimera-only --max-pairs 100

# Custom output file with visualization
poetry run python -m benchmarks.llm_compare --outfile my_results.csv --max-pairs 25
```

**New in v0.3.0 - Memory Optimization:**
- **ðŸ§  Streaming Dataset Loading**: Automatically handles large datasets (10k+ pairs) with minimal memory usage
- **ðŸ“Š Smart Thresholds**: Uses regular loading for small datasets (<100 pairs), streaming for large ones
- **â™»ï¸ Memory Management**: Automatic garbage collection and memory cleanup between chunks
- **ðŸ¼ Pandas Integration**: Efficient CSV chunking with fallback to standard library
- **ðŸ“ˆ Scalability**: Can process 10x larger datasets with same memory footprint

**Benchmark Features:**
- **Multiple models**: Support for gpt-4o, gpt-4o-mini, and other OpenAI models
- **Performance comparison**: Speed (ms/pair) and accuracy metrics
- **Agreement analysis**: How often Kimera and GPT-4o agree
- **CSV export**: Detailed results with reasoning for each pair
- **Matplotlib visualization**: Automatic bar charts and pie charts
- **Rate limiting**: Respects OpenAI API limits with exponential backoff
- **Confidence scoring**: Both systems provide confidence levels
- **Kimera-only mode**: Test without API key for development
- **ðŸš€ Async OpenAI calls**: 5-10Ã— faster GPT-4o benchmarks with concurrent requests
- **âš¡ Smart parallelization**: Auto-detects optimal worker counts for Kimera

**New in v0.7.1 - Echo-Form Implementation:**
- **ðŸ” Enhanced Observability**: Every geoid now carries the exact trimmed text that was embedded
- **ðŸ”— Stable Hashing**: Deterministic geoid IDs based on `sha256(lang + echo)` instead of random UUIDs
- **ðŸ§¹ Whitespace Deduplication**: Leading/trailing spaces no longer create different embeddings
- **ðŸ“Š Explorer Enhancement**: Optional Echo columns in the web explorer for debugging
- **âš¡ Cache Efficiency**: Improved cache hit rates through consistent text normalization

**Testing the Implementation:**
```bash
# Test the fixes
poetry run python test_fixes.py

# Run full test suite
poetry run pytest -q

# Test streaming functionality
poetry run python test_streaming_benchmark.py

# Quick validation
poetry run python run_validation.py
```

**Async Performance Optimization (v0.5.0+):**
```bash
# Install async dependencies
poetry add httpx

# Sync mode (baseline) - 3-6 minutes for 50 pairs
poetry run python -m benchmarks.llm_compare --max-pairs 50 --model gpt-4o-mini

# Async mode (5-10x faster) - 30-60 seconds for 50 pairs  
poetry run python -m benchmarks.llm_compare --max-pairs 50 --model gpt-4o-mini --async 8

# Large dataset comparison
time poetry run python -m benchmarks.llm_compare --max-pairs 100 --async 12

# Kimera parallel + OpenAI async (optimal performance)
poetry run python -m benchmarks.llm_compare --max-pairs 100 --mp 4 --async 10
```

**Concurrency Guidelines:**
- **Small datasets (â‰¤50 pairs)**: `--async 4-6`
- **Medium datasets (50-200 pairs)**: `--async 8-12`  
- **Large datasets (200+ pairs)**: `--async 10-20`
- **Rate limit aware**: Automatic backoff on 429 responses

Expected benchmark output (with async optimization):
```
ðŸ“‚ Loading dataset: data/contradictions_2k.csv
âœ… Testing 50 pairs (streaming mode activated)

=== Running Kimera Benchmark - Parallel Mode ===
ðŸš€ Using 4 worker processes
  Processed 50/50 pairs

=== Running GPT-4o Benchmark (gpt-4o-mini) - Async Mode ===
ðŸš€ Using 8 concurrent requests
  Processed 50/50 pairs

âœ“ Results saved to: benchmark_results.csv
âœ“ Visualization saved to: benchmark_results.png

============================================================
BENCHMARK SUMMARY
============================================================
Dataset: data/contradictions_2k.csv
Total pairs tested: 50

Kimera Results:
  Time: 2.3s
  Avg per pair: 46.8ms
  Contradictions found: 23
  Avg confidence: 0.742
  Mode: parallel (4 workers)

GPT-4o Results:
  Time: 45.2s
  Avg per pair: 904.1ms
  Contradictions found: 19
  Avg confidence: 0.856
  Mode: async (8 concurrent)

Agreement Analysis:
  Total agreements: 42/50
  Agreement rate: 84.0%
  Kimera-only contradictions: 6
  GPT-4o-only contradictions: 2

Performance:
  Kimera is 19.6x faster than GPT-4o
  Async mode: 5.8x faster than sync GPT-4o
  Memory efficiency: 10x improvement with streaming
```

The benchmark automatically generates a comprehensive visualization:

![Benchmark Summary](benchmark_summary_sample.png)

*Sample 4-panel comparison showing contradictions detected, processing speed, confidence scores, and agreement analysis.*

Dataset Generation
------------------
The `scripts/build_dataset.py` tool can generate contradiction pairs in multiple ways:

**Online mode** (scrapes Wikipedia):
```bash
poetry run python scripts/build_dataset.py --rows 2000 --lang en,fr,ar --mode online --out data/contradictions_2k.csv
```

**Offline mode** (uses static seed data):
```bash
poetry run python scripts/build_dataset.py --rows 200 --mode offline --out data/contradictions_offline.csv
```

**Options:**
- `--rows N`: Number of total rows to generate (split evenly across languages)
- `--lang CODES`: Comma-separated language codes (en, fr, ar supported)
- `--mode online|offline`: Data source mode
- `--out PATH`: Output CSV file path

If `ModuleNotFoundError: kimera` persists, run:
```bash
set PYTHONPATH=%CD%\src;%PYTHONPATH%
```

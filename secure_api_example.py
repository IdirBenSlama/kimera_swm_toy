#!/usr/bin/env python3
"""
Example of secure API key usage with the new metrics infrastructure.
This shows the proper way to use OpenAI API keys safely.
"""
import os
import sys
from pathlib import Path

def check_api_key_security():
    """Check if API key is properly configured."""
    print("üîê API Key Security Check")
    print("-" * 30)
    
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        print("‚ùå No OPENAI_API_KEY environment variable found")
        print("\nTo set it securely:")
        print("Windows: set OPENAI_API_KEY=your-key-here")
        print("Linux/Mac: export OPENAI_API_KEY='your-key-here'")
        return False
    
    if api_key.startswith("sk-or-v1-a54f091e"):
        print("‚ö†Ô∏è  WARNING: You're using the compromised key!")
        print("Please revoke it and generate a new one immediately.")
        return False
    
    if len(api_key) < 20:
        print("‚ö†Ô∏è  API key seems too short. Please check it.")
        return False
    
    # Mask the key for display
    masked_key = api_key[:8] + "..." + api_key[-4:]
    print(f"‚úÖ API key found: {masked_key}")
    print("‚úÖ Key appears to be properly formatted")
    return True

def run_secure_benchmark_example():
    """Example of running benchmark with secure API key."""
    print("\nüöÄ Secure Benchmark Example")
    print("-" * 35)
    
    if not check_api_key_security():
        print("\n‚ùå Cannot proceed without secure API key")
        return False
    
    print("\nExample commands you can now run safely:")
    print()
    
    # Basic benchmark
    print("1. Basic Kimera vs GPT comparison:")
    print("   poetry run python -m benchmarks.llm_compare data/toy_contradictions.csv \\")
    print("       --max-pairs 50 --model gpt-4o-mini")
    print()
    
    # With comprehensive metrics
    print("2. Full metrics analysis:")
    print("   poetry run python -m benchmarks.llm_compare data/toy_contradictions.csv \\")
    print("       --max-pairs 100 --model gpt-4o-mini --stats")
    print()
    
    # With async optimization
    print("3. High-performance async benchmark:")
    print("   poetry run python -m benchmarks.llm_compare data/contradictions_2k.csv \\")
    print("       --max-pairs 500 --model gpt-4o-mini --async 12 --stats")
    print()
    
    # Standalone metrics analysis
    print("4. Analyze existing results:")
    print("   poetry run python -m benchmarks.metric_runner benchmark_results.csv")
    print()
    
    print("All of these will generate:")
    print("  üìä metrics.yaml - Machine-readable performance data")
    print("  üìà metrics_plots.png - ROC/PR curves and visualizations")
    print("  üìù summary_report.md - Human-readable analysis")
    
    return True

def demonstrate_metrics_output():
    """Show what the metrics output looks like."""
    print("\nüìä Expected Metrics Output")
    print("-" * 30)
    
    example_yaml = """
kimera:
  auroc: 0.847
  aupr: 0.723
  f1: 0.681
  accuracy: 0.735
  precision: 0.692
  recall: 0.671
  optimal_threshold: 0.623
  auroc_ci: {lower: 0.821, upper: 0.873}
  f1_ci: {lower: 0.651, upper: 0.711}

gpt:
  auroc: 0.792
  aupr: 0.658
  f1: 0.634
  accuracy: 0.682
  precision: 0.618
  recall: 0.651
  optimal_threshold: 0.547
  auroc_ci: {lower: 0.761, upper: 0.823}
  f1_ci: {lower: 0.598, upper: 0.670}

significance:
  mcnemar:
    statistic: 4.17
    p_value: 0.041
    significant: true
  accuracy_difference:
    kimera: 0.735
    gpt: 0.682
    difference: 0.053
"""
    
    print("Example metrics.yaml content:")
    print(example_yaml)
    
    print("This enables evidence-based optimization:")
    print("  ‚Ä¢ AUROC: Kimera 0.847 vs GPT 0.792 (+0.055)")
    print("  ‚Ä¢ Significance: p=0.041 (statistically significant)")
    print("  ‚Ä¢ Performance: Kimera ~26x faster than GPT")

def main():
    """Main demonstration function."""
    print("üéØ Kimera-SWM v0.7.0 - Secure API Usage Guide")
    print("=" * 55)
    print("Demonstrating proper API key security and metrics usage")
    
    # Check security first
    if not run_secure_benchmark_example():
        print("\nüîß Setup Instructions:")
        print("1. Revoke the exposed API key immediately")
        print("2. Generate a new OpenAI API key")
        print("3. Set it as environment variable")
        print("4. Re-run this script to verify")
        return False
    
    demonstrate_metrics_output()
    
    print("\n" + "=" * 55)
    print("üéâ Ready for secure benchmarking with comprehensive metrics!")
    print("\nThe zetetic principle is now operational:")
    print("Every micro-optimization must cite macro evidence.")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)